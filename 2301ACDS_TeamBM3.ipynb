{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muiruric/Athena_Python/blob/master/2301ACDS_TeamBM3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b85ff0",
      "metadata": {
        "id": "87b85ff0"
      },
      "source": [
        "# Advanced Classification Predict Student Solution\n",
        "\n",
        "© Explore Data Science Academy\n",
        "\n",
        "---\n",
        "### Honour Code\n",
        "\n",
        "I {**2301ACDS_TeamBM3**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
        "\n",
        "Non-compliance with the honour code constitutes a material breach of contract."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af45508",
      "metadata": {
        "id": "2af45508"
      },
      "source": [
        "### Predict Overview: EA - Twitter Sentiment Classification Challenge\n",
        "\n",
        "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
        "\n",
        "\n",
        "\n",
        "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
        "\n",
        "\n",
        "\n",
        "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2eae44",
      "metadata": {
        "id": "fe2eae44"
      },
      "source": [
        "<a id=\"cont\"></a>\n",
        "\n",
        "## Table of Contents\n",
        "<a href=#one>1. Introduction</a>\n",
        "\n",
        "<a href=#two>2. Importing Packages</a>\n",
        "\n",
        "\n",
        "<br><br><br>\n",
        "<a href=#x>X. Model</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89dbc118",
      "metadata": {
        "id": "89dbc118"
      },
      "source": [
        " <a id=\"one\"></a>\n",
        "## 1. Introduction\n",
        "<a href=#cont>Back to Table of Contents</a>\n",
        "\n",
        "### Goal\n",
        "\n",
        "To predict an individual’s belief in climate change based on their tweets!\n",
        "\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "**Where is this data from?**\n",
        "The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43,943 tweets were collected. Each tweet is labelled as one of 4 classes, which are described below.\n",
        "\n",
        "**Class Description**\n",
        "\n",
        "- 2 News: the tweet links to factual news about climate change\n",
        "\n",
        "- 1 Pro: the tweet supports the belief of man-made climate change\n",
        "\n",
        "- 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
        "\n",
        "- -1 Anti: the tweet does not believe in man-made climate change Variable definitions\n",
        "\n",
        "**Features**\n",
        "\n",
        "**sentiment:** Which class a tweet belongs in (refer to Class Description above)\n",
        "\n",
        "**message:** Tweet body\n",
        "\n",
        "**tweetid:** Twitter unique id\n",
        "\n",
        "\n",
        "\n",
        "**The files provided**\n",
        "\n",
        "**train.csv** - You will use this data to train your model.\n",
        "\n",
        "**test.csv** - You will use this data to test your model.\n",
        "\n",
        "**SampleSubmission.csv** - is an example of what your submission file should look like. The order of the rows does not matter, but the names of the tweetid's must be correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71a5761b",
      "metadata": {
        "id": "71a5761b"
      },
      "source": [
        " <a id=\"two\"></a>\n",
        "## 2. Importing Packages\n",
        "<a href=#cont>Back to Table of Contents</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521f04c8",
      "metadata": {
        "id": "521f04c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Libraries for data loading, data manipulation and data visulisation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
        "import string\n",
        "import urllib\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8ed3ca",
      "metadata": {
        "id": "db8ed3ca"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "799b6318",
      "metadata": {
        "id": "799b6318"
      },
      "source": [
        "<a id=\"three\"></a>\n",
        "## 3. Loading the Data\n",
        "<a class=\"anchor\" id=\"1.1\"></a>\n",
        "<a href=#cont>Back to Table of Contents</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc2302e",
      "metadata": {
        "id": "bdc2302e"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/train.csv')\n",
        "df_test = pd.read_csv('/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b8b1d1c",
      "metadata": {
        "scrolled": true,
        "id": "6b8b1d1c",
        "outputId": "eef53864-8fab-4749-e7b6-e417996ae66f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid\n",
              "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
              "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1867214",
      "metadata": {
        "id": "d1867214",
        "outputId": "6146a055-a34d-45a0-922c-ff94db48712e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15819, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11039961",
      "metadata": {
        "id": "11039961",
        "outputId": "f3c5550e-77ed-45f3-edf5-35ead72080e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europe will now be looking to China to make su...</td>\n",
              "      <td>169760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Combine this with the polling of staffers re c...</td>\n",
              "      <td>35326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The scary, unimpeachable evidence that climate...</td>\n",
              "      <td>224985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Karoli @morgfair @OsborneInk @dailykos \\r\\nPu...</td>\n",
              "      <td>476263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
              "      <td>872928</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message  tweetid\n",
              "0  Europe will now be looking to China to make su...   169760\n",
              "1  Combine this with the polling of staffers re c...    35326\n",
              "2  The scary, unimpeachable evidence that climate...   224985\n",
              "3  @Karoli @morgfair @OsborneInk @dailykos \\r\\nPu...   476263\n",
              "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5d7a33",
      "metadata": {
        "id": "5b5d7a33",
        "outputId": "aa1aa7d8-88fa-4f02-f690-0bc2ec55fcca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10546, 2)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc0cb85",
      "metadata": {
        "id": "ccc0cb85",
        "outputId": "3135b995-7766-4545-921e-009a4c6bc700"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "message    0\n",
              "tweetid    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab02b58",
      "metadata": {
        "id": "dab02b58",
        "outputId": "f0b5f1c1-594f-4d37-dbec-6a96146c6118"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment    0\n",
              "message      0\n",
              "tweetid      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d9351c",
      "metadata": {
        "scrolled": true,
        "id": "48d9351c",
        "outputId": "516a0074-78c6-414b-a980-fbb370e0424f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1,  2,  0, -1], dtype=int64)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['sentiment'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cc2c27",
      "metadata": {
        "id": "a1cc2c27"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_train, df_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4276e702",
      "metadata": {
        "id": "4276e702"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d06006",
      "metadata": {
        "id": "66d06006"
      },
      "source": [
        "### Text Cleaning\n",
        "\n",
        "The following are the data cleaning techniques used to preprocess the raw data before conducting analysis.\n",
        "\n",
        "- Removal of retweets and duplicate tweets\n",
        "\n",
        "- Handling of hyperlinks\n",
        "\n",
        "- Remove punctuation and noise\n",
        "\n",
        "- Convert text to lowercase\n",
        "\n",
        "- Handling contractions\n",
        "\n",
        "- Handling emojis and emoticons\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f578f00f",
      "metadata": {
        "id": "f578f00f"
      },
      "outputs": [],
      "source": [
        "# removal of retweets and duplicate tweets\n",
        "df.drop_duplicates(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1247a7",
      "metadata": {
        "id": "5e1247a7"
      },
      "outputs": [],
      "source": [
        "#removing the RT sign\n",
        "def removing_retweet(text):\n",
        "    retweet_pattern = r'RT @\\w+|@\\w+'\n",
        "    cleaned_text = re.sub(retweet_pattern, '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "df['message'] = df['message'].apply(removing_retweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b15fe29",
      "metadata": {
        "id": "4b15fe29"
      },
      "outputs": [],
      "source": [
        "#removal of hyperlinks from the data\n",
        "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
        "subs_url = r'url-web'\n",
        "df['message'] = df['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fdb043e",
      "metadata": {
        "scrolled": true,
        "id": "5fdb043e"
      },
      "outputs": [],
      "source": [
        "#removing the punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuation_removed = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return punctuation_removed\n",
        "df['message'] = df['message'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b363672e",
      "metadata": {
        "id": "b363672e"
      },
      "outputs": [],
      "source": [
        "#conversion of all the text to lowercase\n",
        "def to_lowercase(text):\n",
        "    lowercase = text.lower()\n",
        "    return lowercase\n",
        "\n",
        "df['message'] = df['message'].apply(to_lowercase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4f7688",
      "metadata": {
        "id": "fb4f7688"
      },
      "outputs": [],
      "source": [
        "#removing the emojis\n",
        "import emoji\n",
        "\n",
        "def remove_emojis(text):\n",
        "    cleaned_text = emoji.demojize(text)\n",
        "    return cleaned_text\n",
        "df['message'] = df['message'].apply(remove_emojis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d65cd0",
      "metadata": {
        "id": "87d65cd0"
      },
      "source": [
        "We opt to remove the digits within the data such as the years in order to ensure that numerical tokens are not treated as seperate tokens. It might also help to reduce the vocabuay size and minimize the noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c511ad07",
      "metadata": {
        "id": "c511ad07"
      },
      "outputs": [],
      "source": [
        "#removal of digits such as years\n",
        "def remove_digits(text):\n",
        "    cleaned = re.sub(r'\\d+','', text)\n",
        "    return cleaned\n",
        "df['message']= df['message'].apply(remove_digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b0b014",
      "metadata": {
        "id": "c5b0b014",
        "outputId": "eb4002c9-6367-48f8-da26-a7f675ba4214"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>researchers say we have three years to act on...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>todayinmaker wired   was a pivotal year in the...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>its  and a racist sexist climate change denyi...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid\n",
              "0        1.0  polyscimajor epa chief doesnt think carbon dio...   625221\n",
              "1        1.0  its not like we lack evidence of anthropogenic...   126103\n",
              "2        2.0   researchers say we have three years to act on...   698562\n",
              "3        1.0  todayinmaker wired   was a pivotal year in the...   573736\n",
              "4        1.0   its  and a racist sexist climate change denyi...   466954"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43fb6484",
      "metadata": {
        "id": "43fb6484"
      },
      "source": [
        "## Preparing Text Data for Exploratory Data Analysis (EDA)\n",
        "\n",
        "- Tokenisation\n",
        "\n",
        "- Stemming and Lemmatisation\n",
        "\n",
        "- Removal of stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ca9bd1",
      "metadata": {
        "id": "77ca9bd1"
      },
      "source": [
        "\n",
        "**Tokenisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295d506b",
      "metadata": {
        "id": "295d506b"
      },
      "outputs": [],
      "source": [
        "tokeniser = TreebankWordTokenizer()\n",
        "df['tokens'] = df['message'].apply(tokeniser.tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9adbcd07",
      "metadata": {
        "id": "9adbcd07"
      },
      "source": [
        "**Stemming and Lemitisation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc4deef",
      "metadata": {
        "id": "9cc4deef",
        "outputId": "20bb5134-0db5-4cd7-d748-4f607a65f249"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"stemmer = SnowballStemmer('english')\\n\\ndef mbti_stemmer(words, stemmer):\\n    return [stemmer.stem(word) for word in words]\\n\\ndf['stem'] = df['tokens'].apply(mbti_stemmer, args=(stemmer, ))\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''stemmer = SnowballStemmer('english')\n",
        "\n",
        "def mbti_stemmer(words, stemmer):\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "df['stem'] = df['tokens'].apply(mbti_stemmer, args=(stemmer, ))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886cf096",
      "metadata": {
        "id": "886cf096",
        "outputId": "0d8c9551-85ef-46aa-ec6b-4eb813d25551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [polyscimajor, epa, chief, doesnt, think, carb...\n",
              "1    [its, not, like, we, lack, evidence, of, anthr...\n",
              "2    [researchers, say, we, have, three, years, to,...\n",
              "3    [todayinmaker, wired, was, a, pivotal, year, i...\n",
              "4    [its, and, a, racist, sexist, climate, change,...\n",
              "Name: tokens, dtype: object"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['tokens'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aace95d6",
      "metadata": {
        "id": "aace95d6"
      },
      "outputs": [],
      "source": [
        "#lemmatization\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = []\n",
        "    for word in text:\n",
        "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "df['lemmatized'] = df['tokens'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7216f8e",
      "metadata": {
        "id": "f7216f8e",
        "outputId": "6e4a43dd-b929-4f88-ee11-6945a968187d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    polyscimajor epa chief doesnt think carbon dio...\n",
              "1    it not like we lack evidence of anthropogenic ...\n",
              "2    researcher say we have three year to act on cl...\n",
              "3    todayinmaker wired wa a pivotal year in the wa...\n",
              "4    it and a racist sexist climate change denying ...\n",
              "Name: lemmatized, dtype: object"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['lemmatized'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7667289",
      "metadata": {
        "id": "c7667289",
        "outputId": "fa0d661a-258e-4552-d724-e245082d912d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\colette\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('words')\n",
        "\n",
        "english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "def remove_non_english_words(text):\n",
        "    words = text.split()\n",
        "    english_words_filtered = [word for word in words if word.lower() in english_words and len(word)>1]\n",
        "    cleaned_text = ' '.join(english_words_filtered)\n",
        "    return cleaned_text\n",
        "df['cleaned'] = df['lemmatized'].apply(remove_non_english_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15bb25dc",
      "metadata": {
        "id": "15bb25dc",
        "outputId": "6dd95269-4bb5-4db0-ae0e-3127e388510a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    chief doesnt think carbon dioxide is main caus...\n",
              "1    it not like we lack evidence of anthropogenic ...\n",
              "2    researcher say we have three year to act on cl...\n",
              "3    wired wa pivotal year in the war on climate ch...\n",
              "4    it and racist climate change bigot is leading ...\n",
              "Name: cleaned, dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['cleaned'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a526b5d9",
      "metadata": {
        "id": "a526b5d9"
      },
      "source": [
        "**Remove Stop words**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c37698",
      "metadata": {
        "id": "f9c37698"
      },
      "outputs": [],
      "source": [
        "##insert stopwords code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7be54a14",
      "metadata": {
        "id": "7be54a14"
      },
      "outputs": [],
      "source": [
        "# seperate test data and train data\n",
        "train = df.dropna(subset=['sentiment'])\n",
        "test = df[df['sentiment'].isnull()]\n",
        "test.drop(['sentiment'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c32098",
      "metadata": {
        "id": "b4c32098"
      },
      "source": [
        "### Text feature extraction\n",
        "\n",
        "**Bag of words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc43aa35",
      "metadata": {
        "id": "bc43aa35"
      },
      "outputs": [],
      "source": [
        "def bag_of_words_count(words, word_dict={}):\n",
        "    \"\"\" this function takes in a list of words and returns a dictionary\n",
        "        with each word as a key, and the value represents the number of\n",
        "        times that word appeared\"\"\"\n",
        "    for word in words:\n",
        "        if word in word_dict.keys():\n",
        "            word_dict[word] += 1\n",
        "        else:\n",
        "            word_dict[word] = 1\n",
        "    return word_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8cb7397",
      "metadata": {
        "id": "a8cb7397"
      },
      "outputs": [],
      "source": [
        "sentiment_labels = list(train['sentiment'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9248c3",
      "metadata": {
        "id": "ac9248c3"
      },
      "outputs": [],
      "source": [
        "sentiment = {}\n",
        "for pp in sentiment_labels:\n",
        "    df = train.groupby('sentiment')\n",
        "    sentiment[pp] = {}\n",
        "    for row in df.get_group(pp)['tokens']:\n",
        "        sentiment[pp] = bag_of_words_count(row, sentiment[pp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad90d1e7",
      "metadata": {
        "id": "ad90d1e7"
      },
      "outputs": [],
      "source": [
        "#print(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c7d9210",
      "metadata": {
        "id": "5c7d9210"
      },
      "outputs": [],
      "source": [
        "all_words = set()\n",
        "for pp in sentiment_labels:\n",
        "    for word in sentiment[pp]:\n",
        "        all_words.add(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c861673",
      "metadata": {
        "id": "9c861673"
      },
      "outputs": [],
      "source": [
        "sentiment['all'] = {}\n",
        "for pp in sentiment_labels:\n",
        "    for word in all_words:\n",
        "        if word in sentiment[pp].keys():\n",
        "            if word in sentiment['all']:\n",
        "                sentiment['all'][word] += sentiment[pp][word]\n",
        "            else:\n",
        "                sentiment['all'][word] = sentiment[pp][word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665462be",
      "metadata": {
        "id": "665462be"
      },
      "outputs": [],
      "source": [
        "total_words = sum([v for v in sentiment['all'].values()])\n",
        "total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb880249",
      "metadata": {
        "id": "fb880249"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist([v for v in sentiment['all'].values() if v < 10],bins=10)\n",
        "plt.ylabel(\"# of words\")\n",
        "plt.xlabel(\"word frequency\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e974f09f",
      "metadata": {
        "scrolled": false,
        "id": "e974f09f"
      },
      "outputs": [],
      "source": [
        "print(type(sentiment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3654f9",
      "metadata": {
        "id": "0c3654f9"
      },
      "outputs": [],
      "source": [
        "max_count = 10\n",
        "remaining_word_index = [k for k, v in sentiment['all'].items() if v > max_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2327b3b",
      "metadata": {
        "id": "f2327b3b"
      },
      "outputs": [],
      "source": [
        "print(type(remaining_word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c5f103",
      "metadata": {
        "id": "81c5f103"
      },
      "outputs": [],
      "source": [
        "hm = []\n",
        "for p, p_bow in sentiment.items():\n",
        "    df_bow = pd.DataFrame([(k, v) for k, v in p_bow.items() if k in remaining_word_index], columns=['Word', p])\n",
        "    df_bow.set_index('Word', inplace=True)\n",
        "    hm.append(df_bow)\n",
        "\n",
        "# create one big dataframe\n",
        "df_bow = pd.concat(hm, axis=1)\n",
        "df_bow.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb29a45d",
      "metadata": {
        "id": "cb29a45d"
      },
      "outputs": [],
      "source": [
        "df_bow.sort_values(by='all', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f03058",
      "metadata": {
        "scrolled": true,
        "id": "87f03058"
      },
      "outputs": [],
      "source": [
        "df_bow.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d70e1341",
      "metadata": {
        "id": "d70e1341"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e7bebe",
      "metadata": {
        "id": "61e7bebe"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59602934",
      "metadata": {
        "id": "59602934"
      },
      "outputs": [],
      "source": [
        "train_processed = train[['tweetid','stem','lemma','sentiment']]\n",
        "train_processed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a46906",
      "metadata": {
        "id": "c3a46906"
      },
      "outputs": [],
      "source": [
        "remaining_word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3a7a19",
      "metadata": {
        "scrolled": true,
        "id": "0d3a7a19"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_unnecessary_words(words):\n",
        "    #words = words.lower()\n",
        "    return [x for x in words if x in remaining_word_index]\n",
        "\n",
        "train_processed['stem'] = train_processed['stem'].apply(remove_unnecessary_words)\n",
        "train_processed['stem'].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f837474b",
      "metadata": {
        "id": "f837474b"
      },
      "outputs": [],
      "source": [
        "train_processed['lemma'] = train_processed['lemma'].apply(remove_unnecessary_words)\n",
        "#Remove digits and words containing digits\n",
        "train_processed['lemma'].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba264f07",
      "metadata": {
        "id": "ba264f07"
      },
      "outputs": [],
      "source": [
        "\n",
        "sd = train_processed[['stem','lemma']].head(2)\n",
        "sd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93510a3b",
      "metadata": {
        "id": "93510a3b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5872147",
      "metadata": {
        "id": "b5872147"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d94fff8",
      "metadata": {
        "id": "2d94fff8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0cf97d8f",
      "metadata": {
        "id": "0cf97d8f"
      },
      "source": [
        " <a id=\"x\"></a>\n",
        "## X. Model\n",
        "<a href=#cont>Back to Table of Contents</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b98f9a8",
      "metadata": {
        "id": "8b98f9a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "55fdabfe",
      "metadata": {
        "id": "55fdabfe"
      },
      "source": [
        " <a id=\"#\"></a>\n",
        "## X. Model Evaluation\n",
        "\n",
        "<a href=#cont>Back to Table of Contents</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13558eb",
      "metadata": {
        "id": "d13558eb"
      },
      "source": [
        "Classification Accuracy\n",
        "\n",
        "Logarithmic Loss\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "Area under Curve\n",
        "\n",
        "F1 Score\n",
        "\n",
        "Mean Absolute Error\n",
        "\n",
        "Mean Squared Error\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8d6a46",
      "metadata": {
        "id": "fd8d6a46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7eb88be",
      "metadata": {
        "id": "a7eb88be"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a48ba8",
      "metadata": {
        "id": "c2a48ba8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5f5a04",
      "metadata": {
        "id": "9d5f5a04"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54f31b4",
      "metadata": {
        "id": "e54f31b4"
      },
      "outputs": [],
      "source": [
        "# Saving each metric to add to a dictionary for logging\n",
        "\n",
        "f1 = f1_score(y_test, y_pred, average='micro')\n",
        "precision = precision_score(y_test, y_pred,  average='micro')\n",
        "recall = recall_score(y_test, y_pred,  average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebea90d1",
      "metadata": {
        "id": "ebea90d1"
      },
      "outputs": [],
      "source": [
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd37cd84",
      "metadata": {
        "id": "cd37cd84"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries for the data we want to log\n",
        "\n",
        "params = {\"random_state\": 7,\n",
        "          \"model_type\": \"logreg\",\n",
        "          \"scaler\": \"standard scaler\",\n",
        "          \"param_grid\": str(param_grid),\n",
        "          \"stratify\": True\n",
        "          }\n",
        "metrics = {\"f1\": f1,\n",
        "           \"recall\": recall,\n",
        "           \"precision\": precision\n",
        "           }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e96273ab",
      "metadata": {
        "id": "e96273ab"
      },
      "outputs": [],
      "source": [
        "# Log our parameters and results\n",
        "experiment.log_parameters(params)\n",
        "experiment.log_metrics(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff96468",
      "metadata": {
        "id": "9ff96468"
      },
      "outputs": [],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a531461d",
      "metadata": {
        "id": "a531461d"
      },
      "outputs": [],
      "source": [
        "experiment.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9c25c2",
      "metadata": {
        "id": "cb9c25c2"
      },
      "outputs": [],
      "source": [
        "sample = pd.read_csv('datasets/sample_submission.csv')\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78813635",
      "metadata": {
        "id": "78813635"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}